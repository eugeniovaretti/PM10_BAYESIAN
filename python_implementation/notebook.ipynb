{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmdstanpy import CmdStanModel\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazione dati\n",
    "\n",
    "# PM10\n",
    "dataset = pd.read_csv('../bayesmix/resources/datasets/ts.csv', header=None)\n",
    "dataset_plot = pd.read_csv('../bayesmix/resources/datasets/ts_mean.csv', header=None)\n",
    "print(dataset.shape)\n",
    "print(dataset_plot.shape)\n",
    "T = dataset.shape[1]\n",
    "\n",
    "# Coordinate\n",
    "coords_file = pd.read_csv('../bayesmix/resources/datasets/coord.csv', header=None)\n",
    "print(coords_file.shape)\n",
    "coords_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(dataset)\n",
    "y_plot = np.array(dataset_plot)\n",
    "# y_i_t (rows -> localities; columns->weeks)\n",
    "coords = np.array(coords_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for time series \n",
    "\n",
    "def covf(v, tau): # v is a vector\n",
    "    # Covariance function with time lag tau\n",
    "    cov = 0\n",
    "    # check over tau...\n",
    "    for i in range(tau, len(v)):\n",
    "        cov += (v[i] * v[i-tau])\n",
    "    return (cov / len(v)-tau)\n",
    "\n",
    "\n",
    "def avg_covf(vs, tau): # vs is a matrix\n",
    "    # Average sample mean of covariance functions \n",
    "    # with time lag tau of all the series\n",
    "    avg_cov = 0\n",
    "    # check over tau...\n",
    "    n = vs.shape[0] \n",
    "    for i in range(0, n): # da 0 ad n-1\n",
    "        avg_cov += covf(np.array(vs[i]),tau)\n",
    "\n",
    "    return (avg_cov / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nnig_post(clusdata, rho_0, lam, alpha, beta): \n",
    "    \n",
    "    n = clusdata.shape[0] \n",
    "    T = clusdata.shape[1] \n",
    "    # clusdata is a matrix n*T\n",
    "    \n",
    "    A = n * (T-1) * avg_covf(clusdata[:, 0:(T - 1)],0) + lam\n",
    "    B = n * (T-2) * avg_covf(clusdata[:, 0:(T - 1)],1) + lam*rho_0\n",
    "    C = n * T * avg_covf(clusdata,0) + lam*(rho_0**2) \n",
    "    \n",
    "    # Posterior parameters for sampling from the Normal (Rho (k+1))\n",
    "    rho_post = B / A\n",
    "    lam_post = A\n",
    "    \n",
    "    # Posterior parameters for sampling from the Inverse-Gamma (Sigma_2_h (k+1))\n",
    "    alpha_post = alpha + (n * T)/2\n",
    "    beta_post = beta + 0.5*( C - (B**2)/A )\n",
    "    \n",
    "    # Update Sigma_2_h (k+1)\n",
    "    sig_2_new = tfd.InverseGamma(alpha_post, beta_post).sample()\n",
    "    \n",
    "    # Update Rho_h (k+1)\n",
    "    rho_new = tfd.Normal(rho_post, np.sqrt(sig_2_new)/lam_post).sample() \n",
    "    #print(rho_new)\n",
    "    \n",
    "    return rho_new, sig_2_new\n",
    "\n",
    "\n",
    "def marginal_nnig(y, rho_0, lam, alpha, beta):\n",
    "    \n",
    "    n = 1 # In questo caso\n",
    "    T = len(y)\n",
    "    \n",
    "    A =  (T-1) * covf(y[0:(T - 1)],0) + lam\n",
    "    B =  (T-2) * covf(y[0:(T - 1)],1) + lam * rho_0\n",
    "    C =  T * covf(y,0) + lam*(rho_0**2)\n",
    "    \n",
    "    coeff = math.gamma(alpha + T/2) / \\\n",
    "            ( math.gamma(alpha) * (math.pi*2*alpha)**(T/2) * ( ((beta**T)*A)/((alpha**T)*lam) )**(0.5) )\n",
    "    \n",
    "    res = ( 1 + 1/(2*alpha)*( (alpha/beta)*(C - B**2/A) ) )**(-T/2 -alpha)\n",
    "    \n",
    "    return np.log( coeff*res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "### NEAL - STEP 1 ###\n",
    "#####################\n",
    "def sample_clus_allocs(y, clus_allocs, coords, rho_h, sig_2_h, M, a):\n",
    "    \n",
    "    T = y.shape[1]\n",
    "    # clus_allocs = vector of labels of observation i: c_i\n",
    "    # Unique values are stored in:\n",
    "        # rho_h: vector of Rhos of the clusters labelled by c_i\n",
    "        # sig_h: vector of Sigmas of the clusters labelled by c_i\n",
    "    # M, a: parameters for the computation of the weights \n",
    "        # ------------> These two are to be decided <------------\n",
    "    \n",
    "    #_, n_by_clus=np.unique(clus_allocs,return_counts=True)\n",
    "    # I don't care abut the unique values but i save only\n",
    "    # the number of times the unique value appears.\n",
    "    # n_by_clus = vector saying how many obs there are in label c_i\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "\n",
    "        _, n_by_clus=np.unique(clus_allocs,return_counts=True)\n",
    "                \n",
    "        c_i=clus_allocs[i]\n",
    "        n_by_clus[c_i] -= 1\n",
    "        \n",
    "        # Check if it was a singleton --> i have to delete it\n",
    "        if n_by_clus[c_i]==0:\n",
    "            n_by_clus = np.delete(n_by_clus,c_i) # Vettore delle cardinalità degli S_h^(-i)\n",
    "            rho_h = np.delete(rho_h,c_i)   \n",
    "            sig_2_h = np.delete(sig_2_h,c_i)    \n",
    "            clus_allocs[clus_allocs>c_i]-=1 \n",
    "            # Decreasing the labels greater than c_i in order to have labels in sequence\n",
    "        \n",
    "        K = len(n_by_clus)\n",
    "        \n",
    "        log_probs = np.zeros(K+1)\n",
    "        \n",
    "        # Probability of sitting in a table already existing:\n",
    "        near_clus = np.ones(K) # variabili flag, se trovo una località troppo lontana (>a) diventerà zero\n",
    "        \n",
    "        x_i, y_i = coords[i][0], coords[i][1]\n",
    "        # Cycling to check the distancies for each cluster\n",
    "        for j in range(y.shape[0]):\n",
    "            if ( not( near_clus.any() ) ):\n",
    "                break\n",
    "            else:\n",
    "                if( j!=i ): \n",
    "                    if( near_clus[ clus_allocs[j] ] ):\n",
    "                        x_j, y_j = coords[j][0], coords[j][1]\n",
    "                        if( np.sqrt( (x_i-x_j)**2 + (y_i-y_j)**2 ) > a ):\n",
    "                            near_clus[ clus_allocs[j] ] = 0\n",
    "                        \n",
    "        \n",
    "        for k in range(0, K): \n",
    "            if(near_clus[k]):\n",
    "                loc = np.zeros(T)\n",
    "                diag = np.repeat(1 + rho_h[k]**2,T)\n",
    "                diag[T-1] = 1\n",
    "                inf_diag = sup_diag = np.repeat(-rho_h[k], T-1)\n",
    "                H = np.diag(diag, 0) + np.diag(inf_diag, -1) + np.diag(sup_diag, 1)\n",
    "                Sigma_inv = H / sig_2_h[k]                \n",
    "                Sigma = np.linalg.inv(Sigma_inv)\n",
    "                \n",
    "                E,V = np.linalg.eigh(Sigma)                \n",
    "                scale = np.linalg.cholesky(Sigma) #Sigma_inv\n",
    "                \n",
    "                log_probs[k] = np.log(n_by_clus[k])\n",
    "                lik = tfd.MultivariateNormalTriL(loc, scale)\n",
    "                log_probs[k] += lik.log_prob(y[i])  \n",
    "            else:\n",
    "                log_probs[k] = float('-inf')\n",
    "        \n",
    "        # Probability of sitting in a new table:\n",
    "        log_probs[K] = np.log(M)\n",
    "        marg = marginal_nnig(y[i], rho_0, lam, alpha, beta)\n",
    "        log_probs[K] += marg \n",
    "        \n",
    "        # Sampling a new label\n",
    "        h_new = tfd.Categorical(probs=softmax(log_probs)).sample()  \n",
    "                \n",
    "        clus_allocs[i] = h_new\n",
    "        \n",
    "        if h_new == K:             \n",
    "            # Sampling unique values -> i need to add them to the others\n",
    "            # and also update n_by_clus\n",
    "            # Faccio direttamente dalla prior\n",
    "            rho_new = tfd.Normal(loc=rho_first, scale=sig_2_first/lam).sample(3)\n",
    "            sig_2_new = tfd.InverseGamma(alpha, beta).sample(3)\n",
    "            \n",
    "            rho_h = np.concatenate([rho_h,rho_new])\n",
    "            sig_2_h = np.concatenate([sig_2_h,sig_2_new])\n",
    "            n_by_clus = np.concatenate([n_by_clus,[1]])\n",
    "        else:\n",
    "            n_by_clus[h_new] +=1\n",
    "            \n",
    "    return clus_allocs, rho_h, sig_2_h\n",
    " \n",
    "#####################\n",
    "### NEAL - STEP 2 ###\n",
    "#####################\n",
    "\n",
    "def sample_clus_params(y, clus_allocs, rho_0, lam, alpha, beta): # Anche qui ovviamente rivedi parametri\n",
    "    nclus = len(np.unique(clus_allocs)) # K\n",
    "    # How many unique values in the cluster allocations\n",
    "    \n",
    "    clus_labels=np.unique(clus_allocs) \n",
    "    # Vector where in position h i have label c\n",
    "    \n",
    "    rho_out=np.zeros(nclus)\n",
    "    sig_2_out=np.zeros(nclus)\n",
    "    \n",
    "    for h,clus_id in enumerate(clus_labels): \n",
    "        tmp = sample_nnig_post(y[clus_allocs == clus_id], rho_0, lam, alpha, beta)\n",
    "        rho_out[h]=tmp[0]\n",
    "        sig_2_out[h]=tmp[1]\n",
    "        \n",
    "    return rho_out, sig_2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_gibbs(y, clus_allocs, coords, rho_h, sig_2_h, M, a):\n",
    "        \n",
    "    #####################\n",
    "    ### NEAL - STEP 1 ###\n",
    "    #####################\n",
    "    \n",
    "    clus_allocs, rho_h, sig_2_h = sample_clus_allocs(y, clus_allocs, coords, rho_h, sig_2_h, M, a)\n",
    "    \n",
    "    #####################\n",
    "    ### NEAL - STEP 2 ###\n",
    "    #####################\n",
    "    \n",
    "    rho_h, sig_2_h = sample_clus_params(y, clus_allocs, rho_0, lam, alpha, beta)\n",
    "        \n",
    "    return clus_allocs, rho_h, sig_2_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixed parameters\n",
    "rho_0 = 0.4807468\n",
    "lam = 13625.29\n",
    "alpha = 7.690926\n",
    "beta = 1257.956\n",
    "\n",
    "## Initial values\n",
    "rho_first = rho_0 # media campionaria dei rho\n",
    "sig_2_first = 100\n",
    "\n",
    "# sPPM parameters\n",
    "a = 2 \n",
    "M = 0.05 # Consigliato da paper sPPM caso C2 \n",
    "\n",
    "data = y\n",
    "data_plot = y_plot\n",
    "n = data.shape[0]\n",
    "\n",
    "n_clus_init = 3\n",
    "\n",
    "clus_allocs = np.random.choice(np.arange(n_clus_init), size=y.shape[0])\n",
    "\n",
    "rho_h = tfd.Normal(loc=rho_first, scale=sig_2_first/lam).sample(n_clus_init)\n",
    "sig_2_h = tfd.InverseGamma(alpha, beta).sample(n_clus_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 120\n",
    "nburn = 20\n",
    "\n",
    "#clus_chain = []\n",
    "rho_chain = []\n",
    "sig_2_chain = []\n",
    "clus_chain = np.zeros((niter-nburn, n), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "# MCMC #\n",
    "for i in range(niter):\n",
    "    \n",
    "    print(\"\\r{0} / {1}\".format(i+1, niter), flush=True, end=\" \")\n",
    "\n",
    "    clus_allocs, rho_h, sig_2_h = run_one_gibbs(y, clus_allocs, coords, rho_h, sig_2_h, M, a)\n",
    "    #print(i)\n",
    "    #print(\"Cluster allocations: \")\n",
    "    #print(clus_allocs)\n",
    "    #print(\"Rho_h: \")\n",
    "    #print(rho_h)\n",
    "    #print(\"Sigma_2_h: \")\n",
    "    #print(sig_2_h)\n",
    "    #print(\"__________________________________________________________________________________________\")\n",
    "    \n",
    "    if i >= nburn:\n",
    "        #clus_chain.append(clus_allocs)\n",
    "        clus_chain[i-nburn] = clus_allocs\n",
    "        rho_chain.append(rho_h)\n",
    "        sig_2_chain.append(sig_2_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio file\n",
    "\n",
    "pd.DataFrame(clus_chain).to_csv(f'clus_chain_M_{M}_a_{int(a)}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rho_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sig_2_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nburn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution of the number of cluster\n",
    "nclus_chain = np.zeros((niter-nburn), dtype=int)\n",
    "for i in range(niter-nburn):\n",
    "    nclus_chain[i] = np.max(clus_chain[i]) + 1\n",
    "x_graph, y_graph = np.unique(nclus_chain, return_counts=True)\n",
    "plt.bar(x_graph, y_graph)\n",
    "plt.xticks(x_graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "weeks = np.linspace(1, y.shape[1], 52)\n",
    "list_scode = list(set(dataset.iloc[:,0]))\n",
    "checks = 4 # Se lo volete provare mettete un numero pari grazie\n",
    "clus_indeces = np.random.choice(np.arange(nclus_chain.shape[0]), size=checks)\n",
    "\n",
    "figs, axs = plt.subplots(int(checks/2), 2, sharex=True, figsize=(15, 15))\n",
    "\n",
    "for j in range(checks):\n",
    "\n",
    "    clus_index = clus_indeces[j]\n",
    "    palette = list(sns.color_palette(n_colors=len(np.unique(clus_chain[clus_index]))).as_hex())\n",
    "    col_index = clus_chain[clus_index] # Il colore sarà indicato dalla label del cluster\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(np.unique(col_index))):\n",
    "        labels.append(\"Cluster \" + str(i+1))\n",
    "    \n",
    "    for i, col in enumerate(list_scode):\n",
    "        axs[int(j/2), j%2].plot(weeks, data_plot[i], color=palette[int(col_index[i])], label=labels[int(col_index[i])])\n",
    "        \n",
    "    axs[int(j/2), j%2].set_title(\"MCMC iteration #\" + str(clus_index) + \\\n",
    "                                 \" Number of clusters: \" + str(nclus_chain[clus_index]))\n",
    "\n",
    "plt.savefig('sample.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing only one\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "clus_index = 0\n",
    "palette = list(sns.color_palette(n_colors=len(np.unique(clus_chain[clus_index]))).as_hex())\n",
    "col_index = clus_chain[clus_index] # Il colore sarà indicato dalla label del cluster\n",
    "list_scode = list(set(dataset_plot.iloc[:,0]))\n",
    "\n",
    "labels = []\n",
    "for i in range(len(np.unique(col_index))):\n",
    "    labels.append(\"Cluster \" + str(i+1))\n",
    "\n",
    "weeks = np.linspace(1, y.shape[1], 52)\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"] = [8.5, 6]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "for i, col in enumerate(list_scode):\n",
    "    plt.plot(weeks, data[i], color=palette[int(col_index[i])], label=labels[int(col_index[i])])\n",
    "\n",
    "#plt.legend(labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
